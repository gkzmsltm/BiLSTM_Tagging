{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generic-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strr='abcdef'\n",
    "charlist=[c for c in strr]\n",
    "charlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "documentary-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=''\n",
    "for c in charlist:\n",
    "    s+=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upper-inspector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdef'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excessive-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {'dev': 'data/pos_data/dev.dat',\n",
    "            'test': 'data/pos_data/test.dat',\n",
    "            'train': 'data/pos_data/train.dat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hazardous-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His firm , along with some others , issued new buy recommendations on insurer stocks yesterday .\n",
      "PRP$ NN , IN IN DT NNS , VBN JJ NN NNS IN NN NNS NN .\n"
     ]
    }
   ],
   "source": [
    "filename = filenames['dev']\n",
    "data=[[],[]]\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        pair = [p.strip() for p in lines[i].split('\\t')]\n",
    "        print(pair[0])\n",
    "        print(pair[1])\n",
    "        break\n",
    "#         input_tensor = engdict.tensorFromSentence(pair[0], MAX_LENGTH, device=device)\n",
    "#         target_tensor = posdict.tensorFromSentence(pair[1], MAX_LENGTH, device=device)\n",
    "#         data[0].append(input_tensor)\n",
    "#         data[1].append(target_tensor)\n",
    "# self.x_data = data[0]\n",
    "# self.y_data = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "swiss-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pos_data/dev.dat\n",
      "His firm , along with some others , issued new buy recommendations on insurer stocks yesterday .\n",
      "PRP$ NN , IN IN DT NNS , VBN JJ NN NNS IN NN NNS NN .\n",
      "data/pos_data/test.dat\n",
      "Rockwell International Corp. 's Tulsa unit said it signed a tentative agreement extending its contract with Boeing Co. to provide structural parts for Boeing 's 747 jetliners .\n",
      "NNP NNP NNP POS NNP NN VBD PRP VBD DT JJ NN VBG PRP$ NN IN NNP NNP TO VB JJ NNS IN NNP POS CD NNS .\n",
      "data/pos_data/train.dat\n",
      "Confidence in the pound is widely expected to take another sharp dive if trade figures for September , due for release tomorrow , fail to show a substantial improvement from July and August 's near-record deficits .\n",
      "NN IN DT NN VBZ RB VBN TO VB DT JJ NN IN NN NNS IN NNP , JJ IN NN NN , VB TO VB DT JJ NN IN NNP CC NNP POS JJ NNS .\n"
     ]
    }
   ],
   "source": [
    "from dataload import lang\n",
    "engdict = lang.Lang('eng')\n",
    "posdict = lang.Lang('pos')\n",
    "MAX_LENGTH = 0\n",
    "for filename in filenames.values():\n",
    "    print(filename)\n",
    "    with open(filename, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            pair = [p.strip() for p in lines[i].split('\\t')]\n",
    "            print(pair[0])\n",
    "            print(pair[1])\n",
    "            LENGTH = len(pair[0].split(' '))\n",
    "            if MAX_LENGTH < LENGTH:\n",
    "                MAX_LENGTH = LENGTH\n",
    "            engdict.addSentence(pair[0])\n",
    "            posdict.addSentence(pair[1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-borough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: 'PRP$',\n",
       " 3: 'NN',\n",
       " 4: ',',\n",
       " 5: 'IN',\n",
       " 6: 'DT',\n",
       " 7: 'NNS',\n",
       " 8: 'VBN',\n",
       " 9: 'JJ',\n",
       " 10: '.',\n",
       " 11: 'NNP',\n",
       " 12: 'POS',\n",
       " 13: 'VBD',\n",
       " 14: 'PRP',\n",
       " 15: 'VBG',\n",
       " 16: 'TO',\n",
       " 17: 'VB',\n",
       " 18: 'CD',\n",
       " 19: 'VBZ',\n",
       " 20: 'RB',\n",
       " 21: 'CC'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdict.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plain-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "played-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.tensor(range(12)).view(2,3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noble-breast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  6,  7],\n",
       "        [ 2,  3,  8,  9],\n",
       "        [ 4,  5, 10, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.cat((a[0],a[1]), dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "together-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharBiLSTM(nn.Module):\n",
    "    def __init__(self, num_chars, num_words, char_dim, word_dim, size_hidden, num_poss, num_layers, padding_idx):\n",
    "        super(CharBiLSTM, self).__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_dim, padding_idx=padding_idx)\n",
    "        # self.word_embedding = nn.Embedding(num_words, word_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        self.charlstm = nn.LSTM(char_dim, size_hidden, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(word_dim+size_hidden*2, size_hidden, num_layers=num_layers,\n",
    "#                             batch_first=True, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(size_hidden*2, size_hidden, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.linear = nn.Linear(size_hidden*2, num_poss)\n",
    "\n",
    "    def forward(self, input_seqword, input_seqword_seqchar):\n",
    "        print(input_seqword_seqchar.size())\n",
    "        # batch-seq-chars\n",
    "        print(input_seqword.size())\n",
    "        # batch-seq(1 int)\n",
    "        \n",
    "        batch_seqword_seqchar_charemb = self.char_embedding(input_seqword_seqchar)\n",
    "        print(batch_seqword_seqchar_charemb.size())\n",
    "        # batch-seq-cahr-char_dim\n",
    "        \n",
    "        _size = batch_seqword_seqchar_charemb.size()\n",
    "        batchseqword_seqchar_charemb = batch_seqword_seqchar_charemb.view(-1, _size[2], _size[3])\n",
    "        \n",
    "        _output_seq, (bi_batchseqword_charforback, c_n) = self.charlstm(batchseqword_seqchar_charemb)\n",
    "        print(bi_batchseqword_charforback.size())\n",
    "        # 2(BiDirection)-batch*seq-size_hidden\n",
    "        batchseqword_charforback = torch.cat((bi_batchseqword_charforback[0],bi_batchseqword_charforback[1]), dim=1)\n",
    "        print(batchseqword_charforback.size())\n",
    "        # batch*seq-size_hidden*2(BiDirection)\n",
    "        batch_seqword_charforback = batchseqword_charforback.view(_size[0], _size[1], -1)\n",
    "        print(batch_seqword_charforback.size())\n",
    "        # batch_seq-size_hidden*2(BiDirection)\n",
    "\n",
    "\n",
    "        # batch_seqword_wordemb = self.word_embedding(input_seqword)\n",
    "        # print(batch_seqword_wordemb.size())\n",
    "        # batch-seq-word_dim\n",
    "        \n",
    "        # batch_seqword_wordrep = torch.cat((batch_seqword_wordemb, batch_seqword_charforback), dim=2)\n",
    "        batch_seqword_wordrep = batch_seqword_charforback\n",
    "        print(batch_seqword_wordrep.size())\n",
    "        ## batch-seq-word_dim+size_hidden*2(BiDirection)\n",
    "        # batch-seq-size_hidden*2(BiDirection)\n",
    "        \n",
    "        output_seq, (h_n, c_n) = self.lstm(batch_seqword_wordrep)\n",
    "        print(output_seq.size())\n",
    "        # batch-seq-size_hidden*2(BiDirection)\n",
    "        print(h_n.size())\n",
    "        # 2(BiDirection)-batch-size_hidden\n",
    "\n",
    "        output = self.linear(self.dropout(output_seq))\n",
    "        print(output.size())\n",
    "        # batch-seq-num_poss\n",
    "\n",
    "        return output #, output_seq, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "humanitarian-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = 5\n",
    "num_words = 10\n",
    "char_dim = 10\n",
    "word_dim = 10\n",
    "size_hidden = 10\n",
    "num_poss = 10\n",
    "num_layers = 1\n",
    "padding_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "constitutional-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharBiLSTM(num_chars, num_words, char_dim, word_dim, size_hidden, num_poss, num_layers, padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
