{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opening-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataload import lang, localdata\n",
    "# from tagger.charbilstm import CharBiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "leading-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuous-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21591\n",
      "82\n",
      "46\n",
      "54\n",
      "78\n",
      "4\n",
      "78\n",
      "936\n",
      "2012\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "tag_PAD = lang.tag_PAD\n",
    "tag_UNK = lang.tag_UNK\n",
    "\n",
    "datasets, engdict, posdict = localdata.load_eng_pos(device, strmode=True) # , charmode=True)\n",
    "num_words = engdict.n_words\n",
    "num_chars = engdict.n_chars\n",
    "num_poss = posdict.n_words\n",
    "print(num_words)\n",
    "print(num_chars)\n",
    "print(num_poss)\n",
    "# print(MAX_LENGTH)\n",
    "print(engdict.max_len_char)\n",
    "print(engdict.max_len_word)\n",
    "print(posdict.max_len_char)\n",
    "print(posdict.max_len_word)\n",
    "\n",
    "print(len(datasets['dev'].y_data))\n",
    "print(len(datasets['test'].y_data))\n",
    "print(len(datasets['train'].y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "involved-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engdict.index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "educated-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2chars(word):\n",
    "#     for i in word:\n",
    "#         engdict.index2word[i]\n",
    "#     seqword\n",
    "#     return [c for c]\n",
    "# def word2chars(seqwordidx):\n",
    "#     for i in seqwordidx:\n",
    "#         engdict.index2word[i]\n",
    "#     seqword\n",
    "#     return seqchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(samples, optimizer, criterion):\n",
    "    x_train, y_train = samples\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    o = model(x_train)\n",
    "    o = o.view(-1,o.size(-1))\n",
    "    t = y_train.view(-1)\n",
    "\n",
    "    loss = criterion(o, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convinced-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    list_loss = []\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        loss = iteration(samples, optimizer, criterion)\n",
    "        list_loss.append(loss)\n",
    "    return list_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        o = model(x_train)\n",
    "        o = o.view(-1,o.size(-1))\n",
    "        t = y_train.view(-1)\n",
    "\n",
    "        total_loss += criterion(o, t).item()\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "            # print(batch_idx, loss.item())\n",
    "            \n",
    "    # print(batch_idx, loss.item())\n",
    "    return total_loss / (batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "touched-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model, dataset, batch_size):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    cnt_corr_samples = 0\n",
    "    total_words = 0\n",
    "    cnt_corr_words = 0\n",
    "    c = posdict.n_words\n",
    "    table = torch.zeros(c,c)\n",
    "    \n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        o = model(x_train)\n",
    "        t = y_train.view(-1)\n",
    "\n",
    "        __tagged, tagged = torch.max(o, dim=-1)\n",
    "        \n",
    "        for i in range(y_train.size(0)):\n",
    "            l = y_train[i].tolist()\n",
    "            if tag_PAD in l:\n",
    "                seq_len = l.index(tag_PAD)\n",
    "            else:\n",
    "                seq_len = len(l)\n",
    "            \n",
    "            _y = y_train[i,:seq_len]\n",
    "            _p = tagged[i,:seq_len]\n",
    "            d = _p!=_y\n",
    "            cnt_wrong = _p[d].size()[0]\n",
    "            \n",
    "            # score1_acc_sample\n",
    "            if cnt_wrong == 0:\n",
    "                cnt_corr_samples +=1\n",
    "                \n",
    "            # score2_acc_word\n",
    "            total_words += seq_len\n",
    "            cnt_corr_words += seq_len - cnt_wrong\n",
    "            \n",
    "            # score3_f1\n",
    "            for j in range(seq_len):\n",
    "                table[_y[j],_p[j]] += 1\n",
    "\n",
    "    tp = torch.tensor([table[i,i] for i in range(c)])[2:]\n",
    "    d0sum = table[2:,2:].sum(dim=0)\n",
    "    d1sum = table[2:,2:].sum(dim=1)\n",
    "    allsum = d1sum.sum()\n",
    "    \n",
    "    pr = tp / d0sum\n",
    "    temp = pr != pr\n",
    "    pr[temp] = 0\n",
    "    \n",
    "    re = tp / d1sum\n",
    "    temp = re != re\n",
    "    re[temp] = 0\n",
    "    \n",
    "    f1 = 2 * pr * re / (pr + re)\n",
    "    temp = f1 != f1\n",
    "    f1[temp] = 0\n",
    "    \n",
    "    avg_f1 = (f1 * d1sum).sum() / allsum\n",
    "  \n",
    "    return (cnt_corr_samples / total_samples,\n",
    "            cnt_corr_words / total_words,\n",
    "            avg_f1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "polished-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_losses_scores(model, optimizer, criterion, datasets, batch_size, num_epoch):\n",
    "\n",
    "    dict_losses={'train': [],\n",
    "                 'dev': [],\n",
    "                 'test': []}\n",
    "    \n",
    "    dict_scores={'train': [[],[],[]],\n",
    "                 'dev': [[],[],[]],\n",
    "                 'test': [[],[],[]]}\n",
    "    \n",
    "    for epoch_i in range(num_epoch):\n",
    "        dataloader = DataLoader(datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "        dataloader_dev = DataLoader(datasets['dev'], batch_size=batch_size, shuffle=True)\n",
    "        dataloader_test = DataLoader(datasets['test'], batch_size=batch_size, shuffle=True)\n",
    "        print(f'\\r{epoch_i}', end='')\n",
    "        temp = epoch(model, optimizer, criterion, dataloader)\n",
    "        dict_losses['train'].extend(temp)\n",
    "        dict_losses['dev'].append(evaluate(model, criterion, dataloader_dev))\n",
    "        dict_losses['test'].append(evaluate(model, criterion, dataloader_test))\n",
    "\n",
    "        s_train = scores(model, datasets['train'], 100)\n",
    "        s_dev = scores(model, datasets['dev'], 100)\n",
    "        s_test = scores(model, datasets['test'], 100)\n",
    "        \n",
    "        dict_scores['train'][0].append(s_train[0])\n",
    "        dict_scores['train'][1].append(s_train[1])\n",
    "        dict_scores['train'][2].append(s_train[2])\n",
    "        dict_scores['dev'][0].append(s_dev[0])\n",
    "        dict_scores['dev'][1].append(s_dev[1])\n",
    "        dict_scores['dev'][2].append(s_dev[2])\n",
    "        dict_scores['test'][0].append(s_test[0])\n",
    "        dict_scores['test'][1].append(s_test[1])\n",
    "        dict_scores['test'][2].append(s_test[2])\n",
    "\n",
    "    print()\n",
    "    return dict_losses, dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "consistent-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showParallel(words, tags, targets=None):\n",
    "    if '<PAD>' in words:\n",
    "        length = words.index('<PAD>')\n",
    "    else:\n",
    "        lengths = [len(words),len(tags)]        \n",
    "        if targets is not None:\n",
    "            lengths.append(len(targets))\n",
    "        length = min(lengths)\n",
    "    \n",
    "    if targets is not None:\n",
    "        print(f'{\"words\":20}{\"tags\":8}targets')\n",
    "        for i in range(length):\n",
    "            color = 31 if tags[i] != targets[i] else 0\n",
    "            print(f'\\033[{color}m{words[i]:20}{tags[i]:8}{targets[i]}\\033[0m')\n",
    "    else:\n",
    "        print(f'{\"words\":20}tags')\n",
    "        for i in range(length):\n",
    "            print(f'{words[i]:20}{tags[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "absent-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(model, input_seq):\n",
    "    model.eval()\n",
    "    o = model(input_seq)\n",
    "    __tagged, tagged = torch.max(o, dim=-1)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "classified-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGraph(dict_losses, dict_scores):\n",
    "    plt.figure(figsize=(13,13))\n",
    "\n",
    "    plt.subplot(421)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(range(len(dict_losses['train'])), dict_losses['train'], label='train')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(423)\n",
    "    plt.plot(range(1,num_epoch+1), dict_losses['dev'], label='dev')\n",
    "    plt.plot(range(1,num_epoch+1), dict_losses['test'], label='test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Dev, Test Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['train'][0], label='train')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['dev'][0], label='dev')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['test'][0], label='test')\n",
    "    plt.legend()\n",
    "    plt.title('score1_acc_sample')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['train'][1], label='train')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['dev'][1], label='dev')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['test'][1], label='test')\n",
    "    plt.legend()\n",
    "    plt.title('score2_acc_word')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['train'][2], label='train')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['dev'][2], label='dev')\n",
    "    plt.plot(range(1,num_epoch+1), dict_scores['test'][2], label='test')\n",
    "    plt.legend()\n",
    "    plt.title('score3_f1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Avg F1 Score')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "exciting-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(model, input_seq, target_seq=None):\n",
    "    # input_seq = engdict.tensorFromSentence(input_seq, device)\n",
    "    # input_list = engdict.sentenceFromIndexes(input_seq.tolist())\n",
    "    input_list = input_seq.split()\n",
    "\n",
    "    # print(input_list)\n",
    "    # print(sentenceFromIndexes(posdict, target_seq.tolist()))\n",
    "    output_list = posdict.sentenceFromIndexes(pos_tagging(model, [input_seq]).view(-1).tolist())\n",
    "    \n",
    "    target_list = None\n",
    "    if target_seq is not None:\n",
    "        target_list = posdict.sentenceFromIndexes(target_seq.tolist())\n",
    "\n",
    "#     print(output_list)\n",
    "    showParallel(input_list, output_list, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "solar-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharBiLSTM(nn.Module):\n",
    "    def __init__(self, num_chars, num_words, char_dim, word_dim, size_hidden, num_poss, num_layers, padding_idx):\n",
    "        super(CharBiLSTM, self).__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(num_chars, char_dim, padding_idx=padding_idx)\n",
    "        # self.word_embedding = nn.Embedding(num_words, word_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        self.charlstm = nn.LSTM(char_dim, size_hidden, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(word_dim+size_hidden*2, size_hidden, num_layers=num_layers,\n",
    "#                             batch_first=True, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(size_hidden*2, size_hidden, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.linear = nn.Linear(size_hidden*2, num_poss)\n",
    "\n",
    "#     def forward(self, input_seqword, input_seqword_seqchar):\n",
    "    def forward(self, input_seqword):\n",
    "        # print(input_seqword.size())\n",
    "        # print(input_seqword)\n",
    "        # batch-seq(1 int)\n",
    "        bat = []\n",
    "        for s in input_seqword:\n",
    "            bat.append(engdict.charTensorFromSentence(s, device=device))\n",
    "        input_seqword_seqchar = torch.stack(bat)\n",
    "        # print(input_seqword_seqchar.size())\n",
    "        # batch-seq-chars\n",
    "        \n",
    "        batch_seqword_seqchar_charemb = self.char_embedding(input_seqword_seqchar)\n",
    "        # print(batch_seqword_seqchar_charemb.size())\n",
    "        # batch-seq-cahr-char_dim\n",
    "        \n",
    "        _size = batch_seqword_seqchar_charemb.size()\n",
    "        batchseqword_seqchar_charemb = batch_seqword_seqchar_charemb.view(-1, _size[2], _size[3])\n",
    "        \n",
    "        _output_seq, (bi_batchseqword_charforback, c_n) = self.charlstm(batchseqword_seqchar_charemb)\n",
    "        # print(bi_batchseqword_charforback.size())\n",
    "        # 2(BiDirection)-batch*seq-size_hidden\n",
    "        batchseqword_charforback = torch.cat((bi_batchseqword_charforback[0],bi_batchseqword_charforback[1]), dim=1)\n",
    "        # print(batchseqword_charforback.size())\n",
    "        # batch*seq-size_hidden*2(BiDirection)\n",
    "        batch_seqword_charforback = batchseqword_charforback.view(_size[0], _size[1], -1)\n",
    "        # print(batch_seqword_charforback.size())\n",
    "        # batch-seq-size_hidden*2(BiDirection)\n",
    "\n",
    "\n",
    "        # batch_seqword_wordemb = self.word_embedding(input_seqword)\n",
    "        # print(batch_seqword_wordemb.size())\n",
    "        # batch-seq-word_dim\n",
    "        \n",
    "        # batch_seqword_wordrep = torch.cat((batch_seqword_wordemb, batch_seqword_charforback), dim=2)\n",
    "        batch_seqword_wordrep = batch_seqword_charforback\n",
    "        # print(batch_seqword_wordrep.size())\n",
    "        ## batch-seq-word_dim+size_hidden*2(BiDirection)\n",
    "        # batch-seq-size_hidden*2(BiDirection)\n",
    "        \n",
    "        output_seq, (h_n, c_n) = self.lstm(batch_seqword_wordrep)\n",
    "        # print(output_seq.size())\n",
    "        # batch-seq-size_hidden*2(BiDirection)\n",
    "        # print(h_n.size())\n",
    "        # 2(BiDirection)-batch-size_hidden\n",
    "\n",
    "        output = self.linear(self.dropout(output_seq))\n",
    "        # print(output.size())\n",
    "        # batch-seq-num_poss\n",
    "\n",
    "        return output #, output_seq, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "reasonable-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 100\n",
    "char_dim = 10\n",
    "word_dim = 50\n",
    "size_hidden = 50\n",
    "num_layers = 1\n",
    "size_out = num_poss\n",
    "size_batch = 100\n",
    "# criterion  = nn.CrossEntropyLoss().to(device)\n",
    "criterion  = nn.CrossEntropyLoss(ignore_index=tag_PAD).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "model = CharBiLSTM(num_chars, num_words, char_dim, word_dim, size_hidden, size_out, num_layers, tag_PAD).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epoch = 20\n",
    "st = time.time()\n",
    "dict_losses, dict_scores = train_losses_scores(model, optimizer, criterion, datasets, size_batch, num_epoch)\n",
    "print(f'\\t{time.time()-st:5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(dict_losses, dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type=2\n",
    "for i in range(len(dict_scores[\"train\"][score_type])):\n",
    "    print(f'{dict_scores[\"train\"][score_type][i]:<.5}   {dict_scores[\"dev\"][score_type][i]:<05.4}   {dict_scores[\"test\"][score_type][i]:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets['test']\n",
    "for ii in range(len(testset)):\n",
    "    i = 0\n",
    "    input_seq = testset.x_data[i]\n",
    "    target_seq = testset.y_data[i]\n",
    "    calc(model, input_seq, target_seq)\n",
    "    break\n",
    "#     print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I am your father .'\n",
    "in_sen = torch.tensor(engdict.indexesFromSentence(sen), device=device)\n",
    "\n",
    "sen2 = [4609,   25,   35,  116, 4610, 1562,   74,  115, 4611,   49, 4612, 1456,\n",
    "        4613,  224,  691,    6, 4614,   76,   30, 1742, 4615,  308,  107, 4614,\n",
    "         116, 4616, 4617,   17]\n",
    "in_sen = torch.tensor(sen2, device=device)\n",
    "\n",
    "calc(model, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-cycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-butter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
