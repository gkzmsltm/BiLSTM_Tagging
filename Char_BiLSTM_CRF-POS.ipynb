{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swiss-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataload import lang, localdata\n",
    "from tagger.charbilstmcrf import CharBiLSTMCRF\n",
    "from etc.customUtil import showGraph, showParallel\n",
    "from etc import defaultsetting as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "constitutional-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21591\n",
      "82\n",
      "46\n",
      "54\n",
      "78\n",
      "4\n",
      "78\n",
      "936\n",
      "2012\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "tag_PAD = ds.TOKKEN_PAD_IDX\n",
    "tag_UNK = ds.TOKKEN_UNK_IDX\n",
    "\n",
    "datasets, engdict, posdict = localdata.load_eng_pos(device, strmode=True) # , charmode=True)\n",
    "num_words = engdict.n_words\n",
    "num_chars = engdict.n_chars\n",
    "num_poss = posdict.n_words\n",
    "print(num_words)\n",
    "print(num_chars)\n",
    "print(num_poss)\n",
    "# print(MAX_LENGTH)\n",
    "print(engdict.max_len_char)\n",
    "print(engdict.max_len_word)\n",
    "print(posdict.max_len_char)\n",
    "print(posdict.max_len_word)\n",
    "\n",
    "print(len(datasets['dev'].y_data))\n",
    "print(len(datasets['test'].y_data))\n",
    "print(len(datasets['train'].y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disturbed-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(samples, optimizer):\n",
    "    x_train, y_train = samples\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = model(engdict, device, x_train, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marine-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    list_loss = []\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        loss = iteration(samples, optimizer)\n",
    "        list_loss.append(loss.item())\n",
    "    return list_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "obvious-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        loss = model(engdict, device, x_train, y_train, lossmode=True)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "            # print(batch_idx, loss.item())\n",
    "            \n",
    "    # print(batch_idx, loss.item())\n",
    "    return total_loss / (batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handy-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model, dataset, batch_size):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    num_samples = len(dataset)\n",
    "    cnt_corr_samples = 0\n",
    "    total_words = 0\n",
    "    cnt_corr_words = 0\n",
    "    c = posdict.n_words\n",
    "    table = torch.zeros(c,c)\n",
    "    \n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        tagged = model(engdict, device, x_train, lossmode=False)\n",
    "        tagged = torch.tensor(tagged)\n",
    "        y_train = y_train.to('cpu')\n",
    "        \n",
    "        for i in range(y_train.size(0)):\n",
    "            l = y_train[i].tolist()\n",
    "            if tag_PAD in l:\n",
    "                seq_len = l.index(tag_PAD)\n",
    "            else:\n",
    "                seq_len = len(l)\n",
    "            \n",
    "            _y = y_train[i,:seq_len]\n",
    "            _p = tagged[i,:seq_len]\n",
    "            d = _p!=_y\n",
    "            cnt_wrong = _p[d].size()[0]\n",
    "            \n",
    "            # score1_acc_sample\n",
    "            if cnt_wrong == 0:\n",
    "                cnt_corr_samples +=1\n",
    "                \n",
    "            # score2_acc_word\n",
    "            total_words += seq_len\n",
    "            cnt_corr_words += seq_len - cnt_wrong\n",
    "            \n",
    "            # score3_f1\n",
    "            for j in range(seq_len):\n",
    "                table[_y[j],_p[j]] += 1\n",
    "\n",
    "    table = table.to(device)\n",
    "    tp = torch.tensor([table[i,i] for i in range(c)], device=device)[2:]\n",
    "    d0sum = table[2:,2:].sum(dim=0)\n",
    "    d1sum = table[2:,2:].sum(dim=1)\n",
    "    allsum = d1sum.sum()\n",
    "    \n",
    "    pr = tp / d0sum\n",
    "    temp = pr != pr\n",
    "    pr[temp] = 0\n",
    "    \n",
    "    re = tp / d1sum\n",
    "    temp = re != re\n",
    "    re[temp] = 0\n",
    "    \n",
    "    f1 = 2 * pr * re / (pr + re)\n",
    "    temp = f1 != f1\n",
    "    f1[temp] = 0\n",
    "    \n",
    "    avg_f1 = (f1 * d1sum).sum() / allsum\n",
    "    \n",
    "    return (cnt_corr_samples / num_samples,\n",
    "            cnt_corr_words / total_words,\n",
    "            avg_f1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "separated-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_losses_scores(model, optimizer, datasets, batch_size, num_epoch):\n",
    "\n",
    "    dict_losses={'train': [],\n",
    "                 'dev': [],\n",
    "                 'test': []\n",
    "                }\n",
    "    \n",
    "    dict_scores={'train': [[],[],[]],\n",
    "                 'dev': [[],[],[]],\n",
    "                 'test': [[],[],[]]\n",
    "                }\n",
    "    best_dev_f1 = 0.0\n",
    "    for epoch_i in range(1,num_epoch+1):\n",
    "        dataloader = DataLoader(datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "        dataloader_dev = DataLoader(datasets['dev'], batch_size=batch_size, shuffle=True)\n",
    "        dataloader_test = DataLoader(datasets['test'], batch_size=batch_size, shuffle=True)\n",
    "        print(f'\\r{epoch_i}', end='')\n",
    "        temp = epoch(model, optimizer, dataloader)\n",
    "        dict_losses['train'].extend(temp)\n",
    "        dict_losses['dev'].append(evaluate(model, dataloader_dev))\n",
    "        dict_losses['test'].append(evaluate(model, dataloader_test))\n",
    "\n",
    "        s_train = scores(model, datasets['train'], 100)\n",
    "        s_dev = scores(model, datasets['dev'], 100)\n",
    "        s_test = scores(model, datasets['test'], 100)\n",
    "        \n",
    "        dict_scores['train'][0].append(s_train[0])\n",
    "        dict_scores['train'][1].append(s_train[1])\n",
    "        dict_scores['train'][2].append(s_train[2])\n",
    "        dict_scores['dev'][0].append(s_dev[0])\n",
    "        dict_scores['dev'][1].append(s_dev[1])\n",
    "        dict_scores['dev'][2].append(s_dev[2])\n",
    "        dict_scores['test'][0].append(s_test[0])\n",
    "        dict_scores['test'][1].append(s_test[1])\n",
    "        dict_scores['test'][2].append(s_test[2])\n",
    "        \n",
    "        if s_dev[2] > best_dev_f1:\n",
    "            print(f' epoch Dev F1 score: {best_dev_f1:.6} -> {s_dev[2]:.6}')\n",
    "            best_dev_f1 = s_dev[2]\n",
    "            torch.save(model, 'best_char_bilstm_crf_pos.pt')\n",
    "    print()\n",
    "    return dict_losses, dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "destroyed-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(model, input_seq):\n",
    "    model.eval()\n",
    "    o = model(engdict, device, input_seq, lossmode=False)\n",
    "#     print(o)\n",
    "#     __tagged, tagged = torch.max(o, dim=-1)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contemporary-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(model, input_seq, target_seq=None):\n",
    "    # input_seq = engdict.tensorFromSentence(input_seq, device)\n",
    "    # input_list = engdict.sentenceFromIndexes(input_seq.tolist())\n",
    "    input_list = input_seq.split()\n",
    "\n",
    "    # print(input_list)\n",
    "    # print(sentenceFromIndexes(posdict, target_seq.tolist()))\n",
    "    output_list = posdict.sentenceFromIndexes(pos_tagging(model, [input_seq])[0])\n",
    "    \n",
    "    target_list = None\n",
    "    if target_seq is not None:\n",
    "        target_list = posdict.sentenceFromIndexes(target_seq.tolist())\n",
    "\n",
    "#     print(output_list)\n",
    "    showParallel(input_list, output_list, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "funny-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 100\n",
    "char_dim = 10\n",
    "word_dim = 50\n",
    "size_hidden = 50\n",
    "num_layers = 1\n",
    "size_out = num_poss\n",
    "size_batch = 100\n",
    "# criterion  = nn.CrossEntropyLoss().to(device)\n",
    "# criterion  = nn.CrossEntropyLoss(ignore_index=tag_PAD).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-pencil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch Dev F1 score: 0.0 -> 0.724431\n",
      "2"
     ]
    }
   ],
   "source": [
    "model = CharBiLSTMCRF(num_chars, num_words, char_dim, word_dim, size_hidden, size_out, num_layers, tag_PAD).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epoch = 60\n",
    "st = time.time()\n",
    "dict_losses, dict_scores = train_losses_scores(model, optimizer, datasets, size_batch, num_epoch)\n",
    "print(f'\\t{time.time()-st:5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(dict_losses, dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type=2\n",
    "for i in range(len(dict_scores[\"train\"][score_type])):\n",
    "    print(f'{dict_scores[\"train\"][score_type][i]:<.5}   {dict_scores[\"dev\"][score_type][i]:<05.4}   {dict_scores[\"test\"][score_type][i]:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets['test']\n",
    "for ii in range(len(testset)):\n",
    "    i = 300\n",
    "    input_seq = testset.x_data[i]\n",
    "    target_seq = testset.y_data[i]\n",
    "    calc(model, input_seq, target_seq)\n",
    "    break\n",
    "#     print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I am your father .'\n",
    "# in_sen = torch.tensor(engdict.indexesFromSentence(sen), device=device)\n",
    "\n",
    "# sen2 = [4609,   25,   35,  116, 4610, 1562,   74,  115, 4611,   49, 4612, 1456,\n",
    "#         4613,  224,  691,    6, 4614,   76,   30, 1742, 4615,  308,  107, 4614,\n",
    "#          116, 4616, 4617,   17]\n",
    "# in_sen = torch.tensor(sen2, device=device)\n",
    "\n",
    "calc(model, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-parallel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best_char_bilstm_crf_pos.pt')\n",
    "s_dev = scores(best_model, datasets['dev'], 100)\n",
    "s_test = scores(best_model, datasets['test'], 100)\n",
    "print(s_dev)\n",
    "print(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-washer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
