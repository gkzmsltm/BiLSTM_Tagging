{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lightweight-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataload import lang, localdata\n",
    "from tagger.bilstm import BiLSTM\n",
    "from etc.customUtil import showGraph, showParallel\n",
    "from etc import defaultsetting as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "asian-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21591\n",
      "82\n",
      "46\n",
      "54\n",
      "78\n",
      "4\n",
      "78\n",
      "936\n",
      "2012\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "tag_PAD = ds.TOKKEN_PAD_IDX\n",
    "tag_UNK = ds.TOKKEN_UNK_IDX\n",
    "\n",
    "datasets, engdict, posdict = localdata.load_eng_pos(device) # , charmode=True)\n",
    "num_words = engdict.n_words\n",
    "num_chars = engdict.n_chars\n",
    "num_poss = posdict.n_words\n",
    "print(num_words)\n",
    "print(num_chars)\n",
    "print(num_poss)\n",
    "# print(MAX_LENGTH)\n",
    "print(engdict.max_len_char)\n",
    "print(engdict.max_len_word)\n",
    "print(posdict.max_len_char)\n",
    "print(posdict.max_len_word)\n",
    "\n",
    "print(len(datasets['dev'].y_data))\n",
    "print(len(datasets['test'].y_data))\n",
    "print(len(datasets['train'].y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "underlying-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(samples, optimizer, criterion):\n",
    "    x_train, y_train = samples\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    o = model(x_train)\n",
    "    o = o.view(-1,o.size(-1))\n",
    "    t = y_train.view(-1)\n",
    "\n",
    "    loss = criterion(o, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "genetic-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    list_loss = []\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        loss = iteration(samples, optimizer, criterion)\n",
    "        list_loss.append(loss)\n",
    "    return list_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smart-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "\n",
    "        o = model(x_train)\n",
    "        o = o.view(-1,o.size(-1))\n",
    "        t = y_train.view(-1)\n",
    "\n",
    "        total_loss += criterion(o, t).item()\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "            # print(batch_idx, loss.item())\n",
    "            \n",
    "    # print(batch_idx, loss.item())\n",
    "    return total_loss / (batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "progressive-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model, dataset, batch_size):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    cnt_corr_samples = 0\n",
    "    total_words = 0\n",
    "    cnt_corr_words = 0\n",
    "    c = posdict.n_words\n",
    "    table = torch.zeros(c,c)\n",
    "    \n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        o = model(x_train)\n",
    "        t = y_train.view(-1)\n",
    "\n",
    "        __tagged, tagged = torch.max(o, dim=-1)\n",
    "        \n",
    "        for i in range(x_train.size(0)):\n",
    "            l = y_train[i].tolist()\n",
    "            if tag_PAD in l:\n",
    "                seq_len = l.index(tag_PAD)\n",
    "            else:\n",
    "                seq_len = len(l)\n",
    "            \n",
    "            _y = y_train[i,:seq_len]\n",
    "            _p = tagged[i,:seq_len]\n",
    "            d = _p!=_y\n",
    "            cnt_wrong = _p[d].size()[0]\n",
    "            \n",
    "            # score1_acc_sample\n",
    "            if cnt_wrong == 0:\n",
    "                cnt_corr_samples +=1\n",
    "                \n",
    "            # score2_acc_word\n",
    "            total_words += seq_len\n",
    "            cnt_corr_words += seq_len - cnt_wrong\n",
    "            \n",
    "            # score3_f1\n",
    "            for j in range(seq_len):\n",
    "                table[_y[j],_p[j]] += 1\n",
    "\n",
    "    tp = torch.tensor([table[i,i] for i in range(c)])[2:]\n",
    "    d0sum = table[2:,2:].sum(dim=0)\n",
    "    d1sum = table[2:,2:].sum(dim=1)\n",
    "    allsum = d1sum.sum()\n",
    "    \n",
    "    pr = tp / d0sum\n",
    "    temp = pr != pr\n",
    "    pr[temp] = 0\n",
    "    \n",
    "    re = tp / d1sum\n",
    "    temp = re != re\n",
    "    re[temp] = 0\n",
    "    \n",
    "    f1 = 2 * pr * re / (pr + re)\n",
    "    temp = f1 != f1\n",
    "    f1[temp] = 0\n",
    "    \n",
    "    avg_f1 = (f1 * d1sum).sum() / allsum\n",
    "  \n",
    "    return (cnt_corr_samples / total_samples,\n",
    "            cnt_corr_words / total_words,\n",
    "            avg_f1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlimited-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_losses_scores(model, optimizer, criterion, datasets, batch_size, num_epoch):\n",
    "\n",
    "    dict_losses={'train': [],\n",
    "                 'dev': [],\n",
    "                 'test': []\n",
    "                }\n",
    "    \n",
    "    dict_scores={'train': [[],[],[]],\n",
    "                 'dev': [[],[],[]],\n",
    "                 'test': [[],[],[]]\n",
    "                }\n",
    "    best_dev_f1 = 0.0\n",
    "    for epoch_i in range(1,num_epoch+1):\n",
    "        dataloader = DataLoader(datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "        dataloader_dev = DataLoader(datasets['dev'], batch_size=len(datasets['dev']), shuffle=True)\n",
    "        dataloader_test = DataLoader(datasets['test'], batch_size=len(datasets['test']), shuffle=True)\n",
    "        print(f'\\r{epoch_i}', end='')\n",
    "        temp = epoch(model, optimizer, criterion, dataloader)\n",
    "        dict_losses['train'].extend(temp)\n",
    "        dict_losses['dev'].append(evaluate(model, criterion, dataloader_dev))\n",
    "        dict_losses['test'].append(evaluate(model, criterion, dataloader_test))\n",
    "\n",
    "        s_train = scores(model, datasets['train'], 100)\n",
    "        s_dev = scores(model, datasets['dev'], 100)\n",
    "        s_test = scores(model, datasets['test'], 100)\n",
    "        \n",
    "        dict_scores['train'][0].append(s_train[0])\n",
    "        dict_scores['train'][1].append(s_train[1])\n",
    "        dict_scores['train'][2].append(s_train[2])\n",
    "        dict_scores['dev'][0].append(s_dev[0])\n",
    "        dict_scores['dev'][1].append(s_dev[1])\n",
    "        dict_scores['dev'][2].append(s_dev[2])\n",
    "        dict_scores['test'][0].append(s_test[0])\n",
    "        dict_scores['test'][1].append(s_test[1])\n",
    "        dict_scores['test'][2].append(s_test[2])\n",
    "\n",
    "        if s_dev[2] > best_dev_f1:\n",
    "            print(f' epoch Dev F1 score: {best_dev_f1:.6} -> {s_dev[2]:.6}')\n",
    "            best_dev_f1 = s_dev[2]\n",
    "            torch.save(model, 'best_bilstm_pos.pt')\n",
    "    print()\n",
    "    return dict_losses, dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addressed-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(model, input_seq):\n",
    "    model.eval()\n",
    "    o = model(input_seq)\n",
    "    __tagged, tagged = torch.max(o, dim=-1)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "loving-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(model, input_seq, target_seq=None):\n",
    "    input_list = engdict.sentenceFromIndexes(input_seq.tolist())\n",
    "#     print(input_list)\n",
    "    # print(sentenceFromIndexes(posdict, target_seq.tolist()))\n",
    "    output_list = posdict.sentenceFromIndexes(pos_tagging(model, input_seq.unsqueeze(0)).view(-1).tolist())\n",
    "    \n",
    "    target_list = None\n",
    "    if target_seq is not None:\n",
    "        target_list = posdict.sentenceFromIndexes(target_seq.tolist())\n",
    "\n",
    "#     print(output_list)\n",
    "    showParallel(input_list, output_list, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hundred-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "size_hidden = 50\n",
    "num_layers = 1\n",
    "size_out = num_poss\n",
    "size_batch = 100\n",
    "# criterion  = nn.CrossEntropyLoss().to(device)\n",
    "criterion  = nn.CrossEntropyLoss(ignore_index=tag_PAD).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acknowledged-christmas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch Dev F1 score: 0.0 -> 0.887768\n",
      "2 epoch Dev F1 score: 0.887768 -> 0.932619\n",
      "3 epoch Dev F1 score: 0.932619 -> 0.944704\n",
      "4 epoch Dev F1 score: 0.944704 -> 0.949197\n",
      "5 epoch Dev F1 score: 0.949197 -> 0.951221\n",
      "6 epoch Dev F1 score: 0.951221 -> 0.952543\n",
      "9 epoch Dev F1 score: 0.952543 -> 0.953163\n",
      "10 epoch Dev F1 score: 0.953163 -> 0.95318\n",
      "12 epoch Dev F1 score: 0.95318 -> 0.953656\n",
      "15 epoch Dev F1 score: 0.953656 -> 0.953939\n",
      "23 epoch Dev F1 score: 0.953939 -> 0.954253\n",
      "24 epoch Dev F1 score: 0.954253 -> 0.954365\n",
      "39 epoch Dev F1 score: 0.954365 -> 0.954939\n",
      "42"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-af26ae5c03c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdict_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_losses_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\t{time.time()-st:5}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2537f649bfa9>\u001b[0m in \u001b[0;36mtrain_losses_scores\u001b[1;34m(model, optimizer, criterion, datasets, batch_size, num_epoch)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdict_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0ms_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0ms_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dev'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0ms_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-79a1cae8da90>\u001b[0m in \u001b[0;36mscores\u001b[1;34m(model, dataset, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# score3_f1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BiLSTM(num_words, embedding_dim, size_hidden, size_out, num_layers, tag_PAD).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epoch = 60\n",
    "st = time.time()\n",
    "dict_losses, dict_scores = train_losses_scores(model, optimizer, criterion, datasets, size_batch, num_epoch)\n",
    "print(f'\\t{time.time()-st:5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(dict_losses, dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type=2\n",
    "for i in range(len(dict_scores[\"train\"][score_type])):\n",
    "    print(f'{dict_scores[\"train\"][score_type][i]:<.5}   {dict_scores[\"dev\"][score_type][i]:<05.4}   {dict_scores[\"test\"][score_type][i]:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets['test']\n",
    "for ii in range(len(testset)):\n",
    "    i = 300\n",
    "    input_seq = testset.x_data[i]\n",
    "    target_seq = testset.y_data[i]\n",
    "    calc(model, input_seq, target_seq)\n",
    "    break\n",
    "#     print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I am your father .'\n",
    "in_sen = torch.tensor(engdict.indexesFromSentence(sen), device=device)\n",
    "\n",
    "sen2 = [4609,   25,   35,  116, 4610, 1562,   74,  115, 4611,   49, 4612, 1456,\n",
    "        4613,  224,  691,    6, 4614,   76,   30, 1742, 4615,  308,  107, 4614,\n",
    "         116, 4616, 4617,   17]\n",
    "in_sen2 = torch.tensor(sen2, device=device)\n",
    "\n",
    "calc(model, in_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-tablet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best_bilstm_pos.pt')\n",
    "s_dev = scores(best_model, datasets['dev'], 100)\n",
    "s_test = scores(best_model, datasets['test'], 100)\n",
    "print(s_dev)\n",
    "print(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
